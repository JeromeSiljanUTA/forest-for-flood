{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c29cce31-aefb-4e17-9b80-63064541af49",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce056f7-d70b-40e5-8d99-6035d5221e3b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Establish Spark Context\n",
    "`local[*]` lets pyspark know that we're using only one machine, as opposed to a set of machines in a cluster, etc.\n",
    "\n",
    "The * tells `pyspark` to use all the cores on your machine.\n",
    "\n",
    "It's important to not run more than once because once it's running, you'd have to kill the old SparkContext and start again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43a4197a-0ff6-4681-bd61-a3b18afcda7a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/04 16:28:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8476690-6060-4f54-a6af-ae52ad44f6f6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b67b1459-32d5-4b43-971a-c04c4d724e1c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string, _c8: string, _c9: string, _c10: string, _c11: string, _c12: string, _c13: string, _c14: string, _c15: string, _c16: string, _c17: string, _c18: string, _c19: string, _c20: string, _c21: string, _c22: string, _c23: string, _c24: string, _c25: string, _c26: string, _c27: string, _c28: string, _c29: string, _c30: string, _c31: string, _c32: string, _c33: string, _c34: string, _c35: string, _c36: string, _c37: string, _c38: string, _c39: string, _c40: string, _c41: string, _c42: string, _c43: string, _c44: string]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.csv('../data/NFIP/nfip-flood-policies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "707a41d8-9dcb-4cfc-8af0-0795872e8acb",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/04 16:29:25 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(_c0='agriculturestructureindicator', _c1='basefloodelevation', _c2='basementenclosurecrawlspacetype', _c3='cancellationdateoffloodpolicy', _c4='censustract', _c5='condominiumindicator', _c6='construction', _c7='countycode', _c8='crsdiscount', _c9='deductibleamountinbuildingcoverage', _c10='deductibleamountincontentscoverage', _c11='elevatedbuildingindicator', _c12='elevationcertificateindicator', _c13='elevationdifference', _c14='federalpolicyfee', _c15='floodzone', _c16='hfiaasurcharge', _c17='houseofworshipindicator', _c18='latitude', _c19='locationofcontents', _c20='longitude', _c21='lowestadjacentgrade', _c22='lowestfloorelevation', _c23='nonprofitindicator', _c24='numberoffloorsininsuredbuilding', _c25='obstructiontype', _c26='occupancytype', _c27='originalconstructiondate', _c28='originalnbdate', _c29='policycost', _c30='policycount', _c31='policyeffectivedate', _c32='policyterminationdate', _c33='policytermindicator', _c34='postfirmconstructionindicator', _c35='primaryresidenceindicator', _c36='propertystate', _c37='reportedzipcode', _c38='ratemethod', _c39='regularemergencyprogramindicator', _c40='reportedcity', _c41='smallbusinessindicatorbuilding', _c42='totalbuildinginsurancecoverage', _c43='totalcontentsinsurancecoverage', _c44='totalinsurancepremiumofthepolicy')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "python",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3 (ipykernel)",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": {
    "debugger": true
   },
   "name": "python3"
  },
  "name": "20230404-jds-pyspark-setup.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
